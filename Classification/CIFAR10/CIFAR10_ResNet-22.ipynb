{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a33215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要添加一个新单元，输入 '# %%'\n",
    "# 要添加一个新的标记单元，输入 '# %% [markdown]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c92b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== load data =============================\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "classCount = 10\n",
    "batchSize = 64\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# random preview data augmentation effect\n",
    "def randShowGenImage(imGenerator):\n",
    "    [x, y] = imGenerator.next()\n",
    "    for i in range(len(y)):\n",
    "        img = x[i]\n",
    "        label = class_names[y[i]]\n",
    "        plt.figure(i)\n",
    "        plt.imshow(img)\n",
    "        plt.xlabel(label)\n",
    "\n",
    "def loadData(classCount, batchSize):\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    (imgWidth, imgHeight, channels) = x_train[0].shape\n",
    "\n",
    "    imGenerator = tf.keras.preprocessing.image.ImageDataGenerator( \n",
    "        # featurewise_center=True,\n",
    "        # featurewise_std_normalization=True,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=(0.4, 1.3),\n",
    "        rescale=1./255,\n",
    "        rotation_range=13.0, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        # shear_range=7.0,\n",
    "        zoom_range=(1,1.1),\n",
    "        fill_mode='nearest',\n",
    "        cval=120.0, # constant value\n",
    "        data_format='channels_last', #(samples, height, width, channels)\n",
    "    )\n",
    "    imGenerator.fit(x_train)\n",
    "\n",
    "    # add a channel dimension: (n, imgHeight, imgWidth) -> (n, imgHeight, imgWidth, 1), channel=1 with grey-scale images\n",
    "    y_train = tf.reshape(y_train, shape=(len(y_train))).numpy()\n",
    "    y_test = tf.reshape(y_test, shape=(len(y_test))).numpy()\n",
    "    train_gen = imGenerator.flow(x_train, y_train, batch_size=batchSize)\n",
    "    # train_gen = tf.data.Dataset.from_tensor_slices((x_train/255, y_train)).shuffle(1000).batch(batchSize)\n",
    "    ds_test = tf.data.Dataset.from_tensor_slices((x_test/255, y_test)).batch(128)\n",
    "\n",
    "    return (train_gen, ds_test)\n",
    "\n",
    "(train_gen, ds_test) = loadData(classCount=classCount, batchSize=batchSize) \n",
    "# randShowGenImage(train_gen) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58bb27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 6s 18ms/step - loss: 6.4292 - accuracy: 0.1673\n",
      "[6.429225921630859, 0.1673000007867813]\n",
      "Epoch 31/60\n",
      "782/782 [==============================] - 28s 28ms/step - loss: 1.5603 - accuracy: 0.5108 - val_loss: 1.5087 - val_accuracy: 0.5269\n",
      "\n",
      "Epoch 00031: val_accuracy improved from -inf to 0.52690, saving model to ./Models\\Checkpoint\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbe in position 100: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m ebatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(initial_epoch_batch, \u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 94\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mebatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mebatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     display_history(history)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1230\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1227\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1228\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[1;32m-> 1230\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:413\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    411\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 413\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1368\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1368\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\callbacks.py:1419\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[1;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest \u001b[38;5;241m=\u001b[39m current\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_weights_only:\n\u001b[1;32m-> 1419\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1422\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2260\u001b[0m, in \u001b[0;36mModel.save_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trackable_saver\u001b[38;5;241m.\u001b[39msave(filepath, session\u001b[38;5;241m=\u001b[39msession, options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;66;03m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[39;00m\n\u001b[1;32m-> 2260\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_checkpoint_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2261\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_checkpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_relative_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_model_checkpoint_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py:248\u001b[0m, in \u001b[0;36mupdate_checkpoint_state_internal\u001b[1;34m(save_dir, model_checkpoint_path, all_model_checkpoint_paths, latest_filename, save_relative_paths, all_model_checkpoint_timestamps, last_preserved_timestamp)\u001b[0m\n\u001b[0;32m    242\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSave path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m conflicts with path used for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint state.  Please use a different save path.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    244\u001b[0m                      model_checkpoint_path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Preventing potential read/write race condition by *atomically* writing to a\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# file.\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m \u001b[43mfile_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matomic_write_string_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord_checkpoint_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:648\u001b[0m, in \u001b[0;36matomic_write_string_to_file\u001b[1;34m(filename, contents, overwrite)\u001b[0m\n\u001b[0;32m    646\u001b[0m write_string_to_file(temp_pathname, contents)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 648\u001b[0m   \u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_pathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError:\n\u001b[0;32m    650\u001b[0m   delete_file(temp_pathname)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:607\u001b[0m, in \u001b[0;36mrename\u001b[1;34m(oldname, newname, overwrite)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(v1\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgfile.Rename\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename\u001b[39m(oldname, newname, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Rename or move a file / directory.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m   \u001b[43mrename_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43moldname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:623\u001b[0m, in \u001b[0;36mrename_v2\u001b[1;34m(src, dst, overwrite)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.rename\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename_v2\u001b[39m(src, dst, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    612\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Rename or move a file / directory.\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRenameFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbe in position 100: invalid start byte"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# ==================== train & valid data =============================\n",
    "initial_epoch_batch = 1\n",
    "\n",
    "def residual_block(x, filterNumb, kernel_size:int=3):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    fx = layers.ReLU()(fx)\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, padding='same')(fx)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.ReLU()(fx)\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, padding='same')(fx)\n",
    "    out = layers.Add()([x,fx]) # skip\n",
    "    return out\n",
    "\n",
    "def pooling_residual_block(x, filterNumb, kernel_size:int=3):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    fx = layers.ReLU()(fx)\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, strides=2, padding='same')(fx) # pooling conv with strides 2\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    fx = layers.ReLU()(fx)\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, padding='same')(fx)\n",
    "\n",
    "    x = layers.Conv2D(filterNumb, kernel_size=1, strides=2, padding='same')(x) # pooling conv 1 with strides 2\n",
    "\n",
    "    out = layers.Add()([x,fx]) # skip\n",
    "    return out\n",
    "\n",
    "def create_resnet():\n",
    "    inputs = layers.Input(shape=(32,32,3)) # 32*32\n",
    "    hx = layers.BatchNormalization()(inputs)\n",
    "    hx = layers.ReLU()(hx)\n",
    "    hx = layers.Conv2D(32, kernel_size=1, padding='same')(hx)\n",
    "    hx = residual_block(hx, 32)\n",
    "\n",
    "    hx = pooling_residual_block(hx, 64) # 16*16\n",
    "    hx = residual_block(hx, 64)\n",
    "    \n",
    "    hx = pooling_residual_block(hx, 84) # 8*8\n",
    "    hx = residual_block(hx, 84)\n",
    "    \n",
    "    hx = pooling_residual_block(hx, 84) # 4*4\n",
    "    hx = residual_block(hx, 84)\n",
    "\n",
    "    hx = pooling_residual_block(hx, 128) # 2*2\n",
    "\n",
    "    hx = layers.GlobalAvgPool2D()(hx)\n",
    "    outputs = layers.Dense(10)(hx)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def display_history(history):\n",
    "    history = history.history\n",
    "    if len(history) == 0:\n",
    "        return\n",
    "    plt.figure(0)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "\n",
    "if initial_epoch_batch == 0:\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    opt_fn = tf.keras.optimizers.Adam(0.001)\n",
    "    model = create_resnet()\n",
    "    model.compile(optimizer=opt_fn, loss=loss_fn, metrics=['accuracy'])\n",
    "else:\n",
    "    model = tf.keras.models.load_model('./Models/Checkpoint')\n",
    "    print(model.evaluate(x=ds_test, verbose=1))\n",
    "\n",
    "# start training\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(lambda epochIndex, lr: 0.001 * 0.95 ** epochIndex)\n",
    "save_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    './Models/Checkpoint', monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "    save_weights_only=True, mode='max', save_freq='epoch'\n",
    ")\n",
    "\n",
    "ebatch = 30\n",
    "for i in range(initial_epoch_batch, 20):\n",
    "    history = model.fit(x=train_gen, epochs=(i+1)*ebatch, initial_epoch=i*ebatch, \n",
    "                        callbacks=[lr_cb, save_cb], \n",
    "                        validation_data=ds_test, validation_freq=1, verbose=1)\n",
    "    display_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224ad27",
   "metadata": {
    "incorrectly_encoded_metadata": "===================================",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load best check point and save model\n",
    "import tensorflow as tf\n",
    "filePath = './Models/CIFAR10_ResNet-8_0.8992.h5'\n",
    "model = tf.keras.models.load_model('./Models/Checkpoint')\n",
    "model.save(filePath, overwrite=True, include_optimizer=False)\n",
    "print(model.evaluate(x=ds_test, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ad43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== test with opencv =============================\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from common import videoCapture, screenCapture\n",
    "\n",
    "winName = 'CIFAR10 Classifier'\n",
    "canvasSize = 200\n",
    "outputSize = 32\n",
    "model = tf.keras.models.load_model('./Models/CIFAR10_ResNet-8_0.8992.h5', compile=False)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def predict(frame):\n",
    "    img = tf.image.resize(frame, size=(outputSize, outputSize)) / 255\n",
    "    img = tf.reshape(img, shape=(1, outputSize, outputSize, 3))\n",
    "    predict = tf.argmax(model.predict(img), axis=1).numpy()[0]\n",
    "    result = np.copy(frame)\n",
    "    cv2.putText(result, str(class_names[predict]), (40,150), cv2.FONT_HERSHEY_COMPLEX, 1.3, (0,255,0), 2)\n",
    "    cv2.imshow(winName, result)\n",
    "\n",
    "\n",
    "# videoCapture(winName, canvasSize, canvasSize, predict)\n",
    "screenCapture(winName, canvasSize, canvasSize, predict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112da2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "incorrectly_encoded_metadata,-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
