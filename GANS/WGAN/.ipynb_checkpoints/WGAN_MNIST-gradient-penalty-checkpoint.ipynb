{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import glob\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "    train_images = train_images.reshape(train_images.shape + (1,)).astype('float32') / 127.5 - 1\n",
    "    ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    ds = ds.shuffle(len(ds)).batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "======= general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed = tf.random.normal(shape=(9, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_relu(x, useRelu=True):\n",
    "    fx = layers.BatchNormalization()(x)\n",
    "    if useRelu:\n",
    "        fx = layers.LeakyReLU(0.2)(fx)\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(x, filterNumb, kernel_size, strides=1, use_bias=True):\n",
    "    fx = layers.Conv2D(filterNumb, kernel_size, strides, padding='same', \n",
    "                    use_bias=use_bias, kernel_regularizer=L2(0.01))(x)\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_accuracy_fn(fake_pred):\n",
    "    fooled = tf.where(fake_pred > 0, 1.0, 0.0)\n",
    "    return tf.reduce_mean(fooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss_fn(fake_pred):\n",
    "    return -tf.reduce_mean(fake_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_loss_fn(real_pred, fake_pred):\n",
    "    return tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(generator, isShow=True, isSaveFile=False):\n",
    "    grid_size = (3,3)\n",
    "    w, h = grid_size\n",
    "    img_count = w * h\n",
    "    fake_img_batch = generator(test_seed, training=False)\n",
    "    fig = plt.figure(figsize=grid_size, dpi=100)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "    for i in range(img_count):\n",
    "        plt.subplot(w, h, i+1)\n",
    "        plt.imshow(fake_img_batch[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    if isShow: \n",
    "        plt.show()\n",
    "    if isSaveFile:\n",
    "        if not os.path.exists('./Results'):\n",
    "            os.makedirs('./Results')\n",
    "        plt.savefig('./Results/{:06d}.png'.format(dis_opt.iterations.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif():\n",
    "    with imageio.get_writer('./Results/result.gif', mode='I') as writer:\n",
    "        filenames = glob.glob('./Results/*.png')\n",
    "        filenames = sorted(filenames)\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transpose(model, kernels, strides, use_bn=True, activation=None):\n",
    "    model.add(layers.Conv2DTranspose(kernels, (3, 3), strides=strides, \n",
    "                                activation=activation, padding='same', \n",
    "                                use_bias=False))\n",
    "    if use_bn:\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(real_img_batch, fake_img_batch):\n",
    "    alpha = tf.random.normal((real_img_batch.shape[0], 1, 1, 1), 0.0, 1.0)\n",
    "    interpolated = real_img_batch + alpha * (fake_img_batch - real_img_batch)\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        dis_pred = discriminator(interpolated, training=True)\n",
    "\n",
    "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "    grads = gp_tape.gradient(dis_pred, [interpolated])[0]\n",
    "    # 3. Calculate the norm of the gradients.\n",
    "    # norm = tf.norm(grads, ord='euclidean', axis=-1)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= create model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(4*4*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Reshape((4, 4, 256)))\n",
    "    assert model.output_shape == (None, 4, 4, 256)\n",
    "    conv_transpose(model, 128, 2)\n",
    "    assert model.output_shape == (None, 8, 8, 128)\n",
    "    conv_transpose(model, 64, 2)\n",
    "    assert model.output_shape == (None, 16, 16, 64)\n",
    "    conv_transpose(model, 1, 2, activation='tanh', use_bn=False)\n",
    "    assert model.output_shape == (None, 32, 32, 1)\n",
    "    \n",
    "    model.add(layers.Cropping2D((2,2)))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    inputs = layers.Input(shape=(28,28,1)) # 28*28\n",
    "    hx = layers.ZeroPadding2D((2,2))(inputs) # 32*32\n",
    "    hx = conv(hx, 32, kernel_size=5, strides=2)\n",
    "    hx = layers.LeakyReLU(0.2)(hx)\n",
    "    hx = layers.Dropout(0.3)(hx)\n",
    "    hx = conv(hx, 64, kernel_size=5, strides=2)\n",
    "    hx = layers.LeakyReLU(0.2)(hx)\n",
    "    hx = layers.Dropout(0.3)(hx)\n",
    "    hx = conv(hx, 128, kernel_size=5, strides=2)\n",
    "    hx = layers.LeakyReLU(0.2)(hx)\n",
    "    hx = layers.Dropout(0.2)(hx)\n",
    "    hx = layers.Flatten()(hx)\n",
    "    hx = layers.Dense(512)(hx)\n",
    "    hx = layers.LeakyReLU(0.2)(hx)\n",
    "    hx = layers.Dropout(0.2)(hx)\n",
    "    outputs = layers.Dense(1)(hx)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "======= training ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay as LRDecay\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.9)\n",
    "dis_opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.9)\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()\n",
    "checkpoint_path = './Checkpoints/'\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=gen_opt,\n",
    "                                 discriminator_optimizer=dis_opt,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./tb_logs', ignore_errors=True)\n",
    "shutil.rmtree('./Results', ignore_errors=True)\n",
    "gen_summary_writer = tf.summary.create_file_writer('./tb_logs/generator')\n",
    "dis_summary_writer = tf.summary.create_file_writer('./tb_logs/discriminator')\n",
    "gen_mean = tf.keras.metrics.Mean(dtype=tf.float32)\n",
    "dis_mean = tf.keras.metrics.Mean(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ tf.function\n",
    "def train_gen(rand_seed_batch):\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_img_batch = generator(rand_seed_batch, training=True)\n",
    "        fake_pred = discriminator(fake_img_batch, training=True)\n",
    "        gen_loss = gen_loss_fn(fake_pred)\n",
    "    \n",
    "    gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "    gen_accuracy = gen_accuracy_fn(fake_pred)\n",
    "    tf.print(' -- GEN: gen_accuracy', gen_accuracy, ' gen_loss', gen_loss)\n",
    "    return gen_loss, gen_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ tf.function\n",
    "def train_dis(real_img_batch, rand_seed_batch):\n",
    "    with tf.GradientTape() as dis_tape:\n",
    "        fake_img_batch = generator(rand_seed_batch, training=True)\n",
    "        fake_pred = discriminator(fake_img_batch, training=True)\n",
    "        real_pred = discriminator(real_img_batch, training=True)\n",
    "        gd = gradient_penalty(real_img_batch, fake_img_batch)\n",
    "        dis_loss = dis_loss_fn(real_pred, fake_pred) + gd * 10.0\n",
    "        \n",
    "    dis_grad = dis_tape.gradient(dis_loss, discriminator.trainable_variables)\n",
    "    dis_opt.apply_gradients(zip(dis_grad, discriminator.trainable_variables))\n",
    "    gen_accuracy = gen_accuracy_fn(fake_pred)\n",
    "    tf.print(' - DIS: gen_accuracy', gen_accuracy, ' dis_loss', dis_loss)\n",
    "    return dis_loss, gen_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_logs(step, gen_loss, dis_loss):\n",
    "    iterations = dis_opt.iterations.numpy()\n",
    "    percentage = int(np.floor(step/len(ds)*100))\n",
    "    epoch = int(np.floor(iterations/len(ds)))\n",
    "    tf.print('iterations {i}, batch {p}%, epoch {e}'.format(i=iterations, p=percentage, e=epoch))\n",
    "    \n",
    "    if iterations % 40 == 0:\n",
    "        generate_image(generator, isShow=False, isSaveFile=True)\n",
    "        with gen_summary_writer.as_default():\n",
    "            gen_mean(gen_loss)\n",
    "            tf.summary.scalar('loss', gen_mean.result(), step=iterations)\n",
    "        with dis_summary_writer.as_default():\n",
    "            dis_mean(dis_loss)\n",
    "            tf.summary.scalar('loss', dis_mean.result(), step=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_loss = 0\n",
    "gen_loss = 0\n",
    "gen_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    for real_img_batch in ds:\n",
    "        for i in range(2):\n",
    "            rand_seed_batch = tf.random.normal(shape=(batch_size, 100))\n",
    "            dis_loss, gen_accuracy = train_dis(real_img_batch, rand_seed_batch)\n",
    "        rand_seed_batch = tf.random.normal(shape=(batch_size, 100))\n",
    "        gen_loss, gen_accuracy = train_gen(rand_seed_batch)\n",
    "        \n",
    "        step += 1\n",
    "        output_logs(step, gen_loss, dis_loss)\n",
    "    checkpoint.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gif()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
